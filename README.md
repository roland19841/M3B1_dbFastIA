# Projet FastIA â€“ Mise Ã  jour du modÃ¨le IA avec adaptation de la couche dâ€™entrÃ©e

## ğŸ“Œ Contexte gÃ©nÃ©ral
FastIA a enrichi sa base de donnÃ©es avec de nouvelles variables (`nb_enfants`, `quotient_caf`).  
Lâ€™ancien modÃ¨le IA ne pouvait utiliser que l'ancien schÃ©ma, il Ã©tait donc nÃ©cessaire :

- dâ€™adapter **uniquement la couche dâ€™entrÃ©e** du modÃ¨le existant,
- de **conserver tous les poids internes appris** (couches cachÃ©es),
- de **rÃ©entraÃ®ner** le modÃ¨le sur le nouveau schÃ©ma,
- et dâ€™**exposer** ce modÃ¨le mis Ã  jour via une API FastAPI.

Ce README dÃ©crit les choix, les scripts, le pipeline complet, les performances observÃ©es et lâ€™usage de TensorBoard.

---

# 1. Architecture globale du projet

```
data/                   â†’ fichiers CSV sources
app/
 â”œâ”€â”€ ml/
 â”‚    â”œâ”€â”€ tf_train_v1.py          â†’ entraÃ®nement modÃ¨le v1 (ancien schÃ©ma)
 â”‚    â”œâ”€â”€ tf_adapt_to_v2.py       â†’ adaptation du modÃ¨le : Ã©largissement de la couche dâ€™entrÃ©e
 â”‚    â”œâ”€â”€ tf_train_v2.py          â†’ rÃ©entraÃ®nement du modÃ¨le adaptÃ©
 â”‚
 â”œâ”€â”€ routers/
 â”‚      â”œâ”€â”€ predict.py            â†’ API FastAPI exposant POST /predict
 â”‚
 â”œâ”€â”€ main.py                      â†’ point d'entrÃ©e FastAPI
 â”œâ”€â”€ database.py, models.py       â†’ ORM SQLAlchemy
artifacts/
 â”œâ”€â”€ model_v1_old_schema.h5       â†’ modÃ¨le initial
 â”œâ”€â”€ model_v2_new_schema_init.h5  â†’ modÃ¨le adaptÃ© non rÃ©entraÃ®nÃ©
 â”œâ”€â”€ model_v2_new_schema.h5       â†’ modÃ¨le final entraÃ®nÃ©
 â”œâ”€â”€ encoder_v1.pkl               â†’ encoder catÃ©goriel (v1)
 â”œâ”€â”€ scaler_v1.pkl                â†’ scaler anciennes features
 â”œâ”€â”€ extra_scaler_v2.pkl          â†’ scaler nouvelles features
logs/
 â”œâ”€â”€ v1/                          â†’ logs TensorBoard modÃ¨le v1
 â”œâ”€â”€ v2/                          â†’ logs TensorBoard modÃ¨le v2
```

---

# 2. Ã‰tape 1 â€“ EntraÃ®nement du modÃ¨le V1 (ancien schÃ©ma)

## CaractÃ©ristiques du modÃ¨le v1
- EntrÃ©e : 33 features (numÃ©riques + catÃ©gorielles encodÃ©es)
- Architecture :
  - Dense(32, relu)
  - Dense(16, relu)
  - Dense(1)
- Optimiseur : Adam, lr=1e-3
- EarlyStopping + TensorBoard

## Script : `tf_train_v1.py`
Ce script :

1. charge les donnÃ©es depuis la base SQLite,
2. encode les variables catÃ©gorielles (`encoder_v1.pkl`),
3. scale les features (`scaler_v1.pkl`),
4. entraÃ®ne le modÃ¨le,
5. produit la courbe de loss,
6. sauvegarde le modÃ¨le.

## ğŸ“Š RÃ©sultat du modÃ¨le V1

```
[V1] MSE validation finale : 26642.6758
```

Ã‰valuation calculÃ©e **aprÃ¨s restauration des meilleurs poids** via EarlyStopping.

---

# 3. Ã‰tape 2 â€“ Adaptation de la couche dâ€™entrÃ©e (modÃ¨le V2 initial)

## Objectif
- Ajouter **2 nouvelles colonnes** : `nb_enfants`, `quotient_caf`
- Conserver **tous les poids internes de v1**
- Nâ€™augmenter que la dimension dâ€™entrÃ©e

## MÃ©thode
1. Charger le modÃ¨le v1 **sans recompiler**.
2. Lire W_old (poids de la premiÃ¨re Dense) â†’ dimension `(33, 32)`
3. Construire W_new â†’ dimension `(35, 32)` :
   - recopier les 33 lignes existantes,
   - initialiser les 2 nouvelles lignes avec une distribution faible `N(0, 0.01)`
4. Copier les poids des autres couches **Ã  lâ€™identique**.
5. Sauvegarder : `model_v2_new_schema_init.h5`.

---

# 4. Ã‰tape 3 â€“ EntraÃ®nement du modÃ¨le V2 (nouveau schÃ©ma Ã©tendu)

## Pipeline
Le script `tf_train_v2.py` :

1. recharge `encoder_v1.pkl` + `scaler_v1.pkl`,
2. applique la transformation **exactement comme v1** pour les anciennes colonnes,
3. ajoute un **nouveau scaler** pour les nouvelles features (`extra_scaler_v2.pkl`),
4. concatÃ¨ne `[X_old_scaled || new_scaled]`,
5. recharge `model_v2_new_schema_init.h5`,
6. entraÃ®ne le modÃ¨le,
7. logge dans `logs/v2`.

## ğŸ“Š RÃ©sultat du modÃ¨le V2

```
[V2] MSE validation finale : 25663.3750
```

---

# 5. Analyse comparative des performances

| ModÃ¨le | SchÃ©ma | MSE Validation |
|--------|--------|----------------|
| **V1** | ancien (33 features) | **26642.6758** |
| **V2** | Ã©tendu (35 features) | **25663.3750** |

### ğŸ“ˆ Conclusion
- Le modÃ¨le V2 **rÃ©duit lâ€™erreur de validation dâ€™environ 3,7%**  
  â†’ (26642 â†’ 25663)
- Les nouvelles variables `nb_enfants` et `quotient_caf` apportent une **lÃ©gÃ¨re valeur ajoutÃ©e**.
- Les poids internes hÃ©ritÃ©s de V1 ont permis :
  - un entraÃ®nement plus rapide,
  - une stabilisation immÃ©diate de la loss,
  - un comportement cohÃ©rent du rÃ©seau.

---

# 6. Ã‰tape 4 â€“ API FastAPI : exposition du modÃ¨le

## Route principale
MÃ©thode : `POST /predict`

Corps attendu :

```json
{
  "age": 40,
  "height_cm": 175,
  "weight_kg": 80,
  "monthly_income": 2500,
  "credit_history": 3,
  "personal_risk": 0.4,
  "monthly_rent": 800,
  "loan_amount": 150000,
  "sex": "H",
  "sport_licence": "oui",
  "education_level": "licence",
  "region": "Ãle-de-France",
  "smoker": "non",
  "is_french": "oui",
  "family_status": "mariÃ©",
  "nb_enfants": 2,
  "quotient_caf": 750
}
```

RÃ©ponse :

```json
{
  "predicted_credit_score": 12345.67
}
```

---

# 7. Lancer TensorBoard

```bash
tensorboard --logdir logs
```

AccÃ¨s :  
ğŸ‘‰ http://localhost:6006/

---

# 8. Lancer lâ€™API

```bash
uvicorn app.main:app --reload
```

Swagger UI :  
ğŸ‘‰ http://127.0.0.1:8000/docs

---

# 9. Conclusion gÃ©nÃ©rale

- L'approche â€œ**adaptation structurelle**â€ permet dâ€™Ã©tendre un modÃ¨le tout en prÃ©servant son apprentissage.  
- L'hÃ©ritage des poids internes a permis une **meilleure stabilitÃ©** et un **temps d'entraÃ®nement rÃ©duit**.  
- Les nouvelles variables ont un impact positif mais modÃ©rÃ©, suggÃ©rant un potentiel pour :
  - augmenter la profondeur du rÃ©seau,
  - appliquer une meilleure sÃ©lection de features,
  - tester un modÃ¨le non linÃ©aire plus puissant.

---

# 10. Commandes de reproductibilitÃ©

## EntraÃ®ner v1
```bash
python -m app.ml.tf_train_v1
```

## Adapter vers v2
```bash
python -m app.ml.tf_adapt_to_v2
```

## EntraÃ®ner v2
```bash
python -m app.ml.tf_train_v2
```

## Lancer lâ€™API
```bash
uvicorn app.main:app --reload
```

---

# 11. Licence
Projet pÃ©dagogique â€“ FastIA, Module IA & Industrialisation.
